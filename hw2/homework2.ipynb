{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 - Berkeley STAT 157\n",
    "\n",
    "Handout 1/29/2019, due 2/5/2019 by 4pm in Git by committing to your repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-29T22:48:56.163323Z",
     "start_time": "2019-01-29T22:48:53.680455Z"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import nd, autograd, gluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-29T19:41:18.204791Z",
     "start_time": "2019-01-29T19:41:18.201091Z"
    }
   },
   "source": [
    "# 1. Multinomial Sampling\n",
    "\n",
    "Implement a sampler from a discrete distribution from scratch, mimicking the function `mxnet.ndarray.random.multinomial`. Its arguments should be a vector of probabilities $p$. You can assume that the probabilities are normalized, i.e. tha they sum up to $1$. Make the call signature as follows:\n",
    "\n",
    "```\n",
    "samples = sampler(probs, shape) \n",
    "\n",
    "probs   : An ndarray vector of size n of nonnegative numbers summing up to 1\n",
    "shape   : A list of dimensions for the output\n",
    "samples : Samples from probs with shape matching shape\n",
    "```\n",
    "\n",
    "Hints:\n",
    "\n",
    "1. Use `mxnet.ndarray.random.uniform` to get a sample from $U[0,1]$.\n",
    "1. You can simplify things for `probs` by computing the cumulative sum over `probs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[1]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.random.multinomial(nd.array([0.3, 0.3, 0.4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-29T22:48:56.185124Z",
     "start_time": "2019-01-29T22:48:56.165645Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 2.  2.  2.]\n",
       " [ 2.  2.  2.]]\n",
       "<NDArray 2x3 @cpu(0)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sampler(probs, shape):\n",
    "    ## Add your codes here\n",
    "    \n",
    "    cumulative_probs = nd.zeros_like(probs)\n",
    "    for i in range(probs.size):\n",
    "        cumulative_probs[i] = nd.sum(probs[:i+1])    \n",
    "    vals = []\n",
    "    for i in range((int)(nd.prod(nd.array(shape)).asscalar())):\n",
    "        rand = nd.random.uniform()\n",
    "        for j, k in enumerate(cumulative_probs):\n",
    "            if rand <= k:\n",
    "                vals.append(j)\n",
    "    \n",
    "    return nd.array(vals).reshape(shape)\n",
    "\n",
    "# a simple test\n",
    "sampler(nd.array([0.2, 0.3, 0.5]), (2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Central Limit Theorem\n",
    "\n",
    "Let's explore the Central Limit Theorem when applied to text processing. \n",
    "\n",
    "* Download [https://www.gutenberg.org/ebooks/84](https://www.gutenberg.org/files/84/84-0.txt) from Project Gutenberg \n",
    "* Remove punctuation, uppercase / lowercase, and split the text up into individual tokens (words).\n",
    "* For the words `a`, `and`, `the`, `i`, `is` compute their respective counts as the book progresses, i.e. \n",
    "    $$n_\\mathrm{the}[i] = \\sum_{j = 1}^i \\{w_j = \\mathrm{the}\\}$$\n",
    "* Plot the proportions $n_\\mathrm{word}[i] / i$ over the document in one plot.\n",
    "* Find an envelope of the shape $O(1/\\sqrt{i})$ for each of these five words.\n",
    "* Why can we **not** apply the Central Limit Theorem directly? \n",
    "* How would we have to change the text for it to apply? \n",
    "* Why does it still work quite well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-29T22:48:56.200892Z",
     "start_time": "2019-01-29T22:48:56.188145Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 78098)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = gluon.utils.download('https://www.gutenberg.org/files/84/84-0.txt')\n",
    "with open(filename) as f:\n",
    "    book = f.read()\n",
    "import string\n",
    "words = list(map(lambda x: x.strip(string.punctuation).lower(), book.split()))\n",
    "counter = {'the':0, 'a':0, 'and':0, 'i':0, 'is':0}\n",
    "proportions = nd.zeros((5, len(words)))\n",
    "for i, word in enumerate(words):\n",
    "    if word in counter:\n",
    "        counter[word] += 1\n",
    "    proportions[:, i] = nd.array(list(counter.values())) / i\n",
    "proportions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "props = proportions.asnumpy()\n",
    "props = np.nan_to_num(props, copy=False)\n",
    "\n",
    "means = np.mean(props[:, 1:], axis=1)\n",
    "variances = np.var(props[:, 1:], axis=1)\n",
    "print(len(means))\n",
    "print(len(variances))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.xscale(\"log\")\n",
    "for i in range(5):\n",
    "    plt.plot(range(78098), props[i, :])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(5):\n",
    "    plt.semilogx(range(1, 78098), (variances[i]**0.5) * np.power(range(1, 78098), -0.5) + means[i],'r')\n",
    "    plt.semilogx(range(1, 78098), -(variances[i]**0.5) * np.power(range(1, 78098), -0.5) + means[i],'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4329, 1439, 3028, 2766, 330]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "word_arr = np.array(words)\n",
    "[sum(word_arr == word) for word in counter.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot apply the CLT directly because we are not drawing words randomly. Instead, we are using the order in the book. We could instead draw words randomly and then sum the occurences to get a proportion, and if we repeat that multiple times, the average of the proportions should approach the true proportion for each word (by CLT). Right now, we still converge to the true proportion, but that is because we go through all the words in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Denominator-layout notation\n",
    "\n",
    "We used the numerator-layout notation for matrix calculus in class, now let's examine the denominator-layout notation.\n",
    "\n",
    "Given $x, y\\in\\mathbb R$, $\\mathbf x\\in\\mathbb R^n$ and $\\mathbf y \\in \\mathbb R^m$, we have\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial \\mathbf{x}}=\\begin{bmatrix}\n",
    "\\frac{\\partial y}{\\partial x_1}\\\\\n",
    "\\frac{\\partial y}{\\partial x_2}\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\partial y}{\\partial x_n}\n",
    "\\end{bmatrix},\\quad \n",
    "\\frac{\\partial \\mathbf y}{\\partial {x}}=\\begin{bmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x}, \n",
    "\\frac{\\partial y_2}{\\partial x}, \n",
    "\\ldots,\n",
    "\\frac{\\partial y_m}{\\partial x}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf y}{\\partial \\mathbf{x}}\n",
    "=\\begin{bmatrix}\n",
    "\\frac{\\partial \\mathbf y}{\\partial {x_1}}\\\\\n",
    "\\frac{\\partial \\mathbf y}{\\partial {x_2}}\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\partial \\mathbf y}{\\partial {x_3}}\\\\\n",
    "\\end{bmatrix}\n",
    "=\\begin{bmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1}, \n",
    "\\frac{\\partial y_2}{\\partial x_1},\n",
    "\\ldots,\n",
    "\\frac{\\partial y_m}{\\partial x_1}\n",
    "\\\\ \n",
    "\\frac{\\partial y_1}{\\partial x_2},\n",
    "\\frac{\\partial y_2}{\\partial x_2},\n",
    "\\ldots,\n",
    "\\frac{\\partial y_m}{\\partial x_2}\\\\ \n",
    "\\vdots\\\\\n",
    "\\frac{\\partial y_1}{\\partial x_n},\n",
    "\\frac{\\partial y_2}{\\partial x_n},\n",
    "\\ldots,\n",
    "\\frac{\\partial y_m}{\\partial x_n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Questions: \n",
    "\n",
    "1. Assume $\\mathbf  y = f(\\mathbf u)$ and $\\mathbf u = g(\\mathbf x)$, write down the chain rule for $\\frac {\\partial\\mathbf  y}{\\partial\\mathbf x}$\n",
    "2. Given $\\mathbf X \\in \\mathbb R^{m\\times n},\\ \\mathbf w \\in \\mathbb R^n, \\ \\mathbf y \\in \\mathbb R^m$, assume $z = \\| \\mathbf X \\mathbf w - \\mathbf y\\|^2$, compute $\\frac{\\partial z}{\\partial\\mathbf w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $\\frac {\\partial\\mathbf  y}{\\partial\\mathbf x} = \\frac {\\partial\\mathbf  y}{\\partial\\mathbf u} \\frac {\\partial\\mathbf  u}{\\partial\\mathbf x}$\n",
    "\n",
    "2. $z = (Xw - y)^T(Xw - y)$\n",
    "\n",
    "$z = w^TX^TXw - 2w^TX^Ty + y^Ty$\n",
    "\n",
    "$\\frac{\\partial z}{\\partial\\mathbf w} = 2X^TXw - 2X^Ty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Numerical Precision\n",
    "\n",
    "Given scalars `x` and `y`, implement the following `log_exp` function such that it returns a numerically stable version of \n",
    "$$-\\log\\left(\\frac{e^x}{e^x+e^y}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-29T22:48:56.206890Z",
     "start_time": "2019-01-29T22:48:56.202996Z"
    }
   },
   "outputs": [],
   "source": [
    "def log_exp(x, y):\n",
    "    ## add your solution here\n",
    "    return -nd.log(nd.exp(x) / (nd.exp(x) + nd.exp(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your codes with normal inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-29T22:48:56.215579Z",
     "start_time": "2019-01-29T22:48:56.209659Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[ 1.31326175]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = nd.array([2]), nd.array([3])\n",
    "z = log_exp(x, y)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement a function to compute $\\partial z/\\partial x$ and $\\partial z/\\partial y$ with `autograd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-29T22:48:56.223303Z",
     "start_time": "2019-01-29T22:48:56.218056Z"
    }
   },
   "outputs": [],
   "source": [
    "def grad(forward_func, x, y): \n",
    "    x.attach_grad()\n",
    "    y.attach_grad()\n",
    "    with autograd.record():\n",
    "        z = forward_func(x, y)\n",
    "    z.backward()\n",
    "    print('x.grad =', x.grad)\n",
    "    print('y.grad =', y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your codes, it should print the results nicely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-29T22:48:56.267165Z",
     "start_time": "2019-01-29T22:48:56.227035Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad = \n",
      "[-0.7310586]\n",
      "<NDArray 1 @cpu(0)>\n",
      "y.grad = \n",
      "[ 0.7310586]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "grad(log_exp, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But now let's try some \"hard\" inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-29T22:48:56.285842Z",
     "start_time": "2019-01-29T22:48:56.274079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad = \n",
      "[ nan]\n",
      "<NDArray 1 @cpu(0)>\n",
      "y.grad = \n",
      "[ nan]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "x, y = nd.array([50]), nd.array([100])\n",
    "grad(log_exp, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does your code return correct results? If not, try to understand the reason. (Hint, evaluate `exp(100)`). Now develop a new function `stable_log_exp` that is identical to `log_exp` in math, but returns a more numerical stable result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-29T22:48:56.305595Z",
     "start_time": "2019-01-29T22:48:56.293399Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad = \n",
      "[-1.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "y.grad = \n",
      "[ 1.]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "def stable_log_exp(x, y):\n",
    "    xs = x.asscalar()\n",
    "    ys = y.asscalar()\n",
    "    if xs < ys:\n",
    "        denom = x + nd.log(1 + nd.exp(y - x))\n",
    "    else:\n",
    "        denom = y + nd.log(1 + nd.exp(x - y))\n",
    "    return -x + denom\n",
    "\n",
    "grad(stable_log_exp, x, y)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
